# 问题

解决浏览器收藏网址时手动选项分类目录的繁琐。
相关技术： `文本分类`，`特征选择`

# 收集数据
- firefox或者chome的书签导出数据
- 模拟用户浏览器输入

# 输入数据
- 提取正文，过滤噪声

# 分析数据

## 文本预处理

中文分词，然后去除停用词、删除低频词词，进行word->id转换

## 文档模型1: 空间向量模型
`向量空间模型(VSM：Vector Space Model)`, 当文档被表示为文档空间的向量时，就可以通过计算向量之间的相似性来度量文档间的相似性。文本处理中最常用的相似性度量方式是`余弦距离`。

文本挖掘系统采用向量空间模型，用特征词条(T1，T2，…Tn)及其权值Wi代表目标信息，在进行信息匹配时，使用这些特征项评价未知文本与目标样本的相关程度。特征词条及其权值的选取称为目标样本的特征提取，特征提取算法的优劣将直接影响到系统的运行效果。

### TF-IDF加权方法
`Term Frequency/ Inverse Document Frequency`, 一种基于统计学的加权方法。主要思想是：如果一个词或者短语在文章中出现的频率（TF）高，且在其他文章中出现的很少，则认为次词语或者短语具有较好的类别区分能力，适合用于分类。

**词频** 指的是给定的词在该文章中出现的频率，将其对词数归一化以防止其偏向较长的文件。  
**逆文档频率** 是一个词普遍重要性的度量，可以由文件总数/包含该词文件的数目，再取对数得到。

参考：http://www.tfidf.com/

### 向量模型用于计算文章相似度

设D为一个包含m个文档的文档集合Di为第i个文档的特征向量，则有：

**文档集合** : `D={D1，D2，…，Dm}; Di=(di1 di2 ... dij), i=1,2,...m; j=1,2,...n`
其中`dij (i=1,2,...m; j=1,2,...n)`为文档Di中第j个词条tj的权值。它一般被定义为tj在Di中出现的频率tij的函数，例如采用TF-IDF函数，即`dij=tij*log(N/nj)`。其中N是文档数据库中文档总数，nj是文档数据库含有词条tj的文档数目。假设用户给定的文档向量为D，未知的文档向量为q，两者的相似程度可用两向量的夹角余弦来度量，夹角越小说明相似度越高。相似度的计算公式如下：

<img src="http://latex.codecogs.com/svg.latex?cos \theta = \frac{D.q}{\| D \| \|q\|}">


## 文档模型2: 互信息量

一般而言，信道中总是存在着噪声和干扰，信源发出消息x，通过信道后信宿只可能收到由于干扰作用引起的某种变形的y。信宿收到y后推测信源发出x的概率，这一过程可由后验概率p(x|y)来描述。相应地，信源发出x的概率p(x)称为先验概率。我们定义x的后验概率与先验概率比值的对数为y对x的互信息量（简称互信息）。在信息论中互信息量计算公式如下：

<img src="http://latex.codecogs.com/svg.latex?I(x,y) = \log  \frac{p(x|y)}{p(x)}  = \log \frac {p(xy)} {p(x) p(y)}">

通俗理解：`原来我对X有些不确定，不确定性为p(x)，告诉我y后根据推测我对x不确定性变为p(x|y), 这个不确定性的减少量就是x,y之间的互信息I(x,y)=p(x)-p(x|y)`

用文本分类相关变量来解释即：  
`p(x|y)`为类别y的文档中x词出现的概率  
`p(xy)`为出现词x且属于类别y的概率  
`p(x)`,`p(y)`分别为词x、文档y出现的概率

## 文档模型3：概率模型
贝叶斯模型

## 文档模型4：人工神经网络模型（ANN，Atificial Neural Networks）

Wiener和Ng曾分别将神经网络技术应用于文本分类,其输出单元一般是文本特征词项,输出单元表示文本属性类别,神经元的连接权重表示归属依赖度.反向传播(Back Propagation,BP)算法是目前最常用的.

神经网络能够对具有噪声的数据进行处理,对未经训练的数据的模式分类能力较强,但其需要较长的训练时长,对于时间需求度不高的应用更为合适;神经网络中的参数主要依靠人工经验调节.


# 训练数据
- 分类
- 聚类

# 测试迭代
- 数据迭代
- 用户监督，结果人工修正

# 工具

- NLTK python的自然语言处理包

## NL

- LSI潜在语义分析
